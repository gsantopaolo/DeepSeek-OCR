# RunPod Template Configuration
# Copy these values when creating your template in RunPod UI

TEMPLATE_NAME="DeepSeek-OCR vLLM Server"
DOCKER_IMAGE="gsantopaolo/deepseek-ocr:1.0.1"
CONTAINER_DISK_GB=100
VOLUME_DISK_GB=50
VOLUME_MOUNT_PATH="/root/.cache/huggingface"
EXPOSE_HTTP_PORTS="8000"
EXPOSE_TCP_PORTS=""
DOCKER_COMMAND=""
CATEGORY="AI/ML"

# NOTE: RunPod automatically handles:
# - NVIDIA GPU runtime (--runtime=nvidia --gpus=all)
# - Shared memory (--ipc=host)
# You don't need to specify these manually!

# Environment Variables (REQUIRED)
ENV_NVIDIA_VISIBLE_DEVICES="all"
ENV_NVIDIA_DRIVER_CAPABILITIES="compute,utility"
ENV_HF_HOME="/root/.cache/huggingface"
ENV_PYTHONUNBUFFERED="1"
ENV_VLLM_WORKER_MULTIPROC_METHOD="spawn"

# Optional Environment Variables
# ENV_HUGGING_FACE_HUB_TOKEN=""  # For private models
# ENV_VLLM_LOGGING_LEVEL="INFO"

# GPU Requirements
MIN_GPU_VRAM_GB=24
RECOMMENDED_GPU="RTX 4090 or A100 40GB"

# Startup Time
FIRST_RUN_MINUTES=5-10
SUBSEQUENT_RUN_SECONDS=30-60

# API Endpoints (after deployment)
# Health: http://<pod-id>-8000.proxy.runpod.net/health
# API: http://<pod-id>-8000.proxy.runpod.net/v1
# Docs: http://<pod-id>-8000.proxy.runpod.net/docs
